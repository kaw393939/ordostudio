# The 40/60 Method — Balancing Manual Skill-Building with AI-Directed Workflow

## The Principle

> *40% of practice time is done the hard way — building foundational understanding. 60% is done with AI agents — practicing the actual workflow professionals use in production.*

You cannot exercise professional judgment over something you do not understand. An engineer who has never written a SQL query cannot evaluate whether an AI-generated query is correct, efficient, or dangerous. An engineer who has never structured an API cannot assess whether an AI's design recommendation is appropriate.

The 40% manual work is not nostalgia for a pre-AI world. It is the **minimum investment** required to produce informed judgment.

The 60% AI-directed work is not "using AI to make things easier." It is training in the **actual professional workflow** that employers are paying $110K–$350K+ for: context construction, output evaluation, iteration management, and accountability.

---

## How It Works

### The 40%: Building the Judgment Substrate

The manual portion builds:
- **Mental models** of how systems actually work (not how they appear to work)
- **Pattern recognition** for when output looks wrong
- **Debugging skill** for when the agent's work fails
- **Credibility** to evaluate AI output in professional contexts

### The 60%: Practicing the Real Workflow

The AI-directed portion trains:
- **Context construction** — building Context Packs that produce useful AI output
- **Output evaluation** — examining what the AI produced and making informed decisions (accept, reject, modify)
- **Iteration management** — directing the AI toward improvement using domain understanding from the 40%
- **Multi-agent orchestration** — coordinating multiple AI work streams across a project
- **Accountability** — taking responsibility for the final product regardless of who (or what) produced the components

---

## A Practical Session (2 Hours)

### First 45 minutes — Manual Work (~40%)

Practitioners perform foundational tasks to understand what the machine is doing:

- Write a test suite manually for a specific feature
- Run tests, observe failures, debug by reading code
- Understand exactly what "passing" means for this component
- Build the mental model that will inform evaluation

### Next 75 minutes — AI-Directed Work (~60%)

Practitioners direct AI agents using the understanding they just built:

1. **Construct a Context Pack** — feature spec, test requirements, existing codebase context, evaluation criteria
2. **Direct the AI agent** to implement the feature
3. **Evaluate AI output** against the test suite they wrote in the 40% phase
4. **Document in the AI Audit Log** — what was accepted, what was modified, what was rejected and why
5. **Iterate** until the feature passes all tests and meets quality standards

**The critical insight:** The manual portion *gives meaning to* the AI-directed portion. Practitioners are not accepting AI output blindly. They are evaluating it against understanding they built minutes earlier.

---

## Calibrating the Ratio for Your Team

The 40/60 split is a starting point, not a law. The right ratio depends on team maturity:

### Team New to AI (50/40/10)
- **50% manual work** — the team needs more foundational understanding
- **40% AI-directed work** — smaller scope, more guidance
- **10% reflection** — AI Audit Log review, lessons learned

Use this when: the team has never worked with AI tools systematically, or when AI output quality is consistently poor.

### Team with Some AI Experience (40/60)
- **40% manual work** — reinforcing judgment on new domains
- **60% AI-directed work** — standard Context Pack workflow

Use this when: the team has basic AI tool familiarity but lacks a systematic workflow.

### Experienced AI Team (25/65/10)
- **25% manual work** — targeted to areas where judgment gaps appear
- **65% AI-directed work** — full multi-agent orchestration
- **10% advanced evaluation** — building custom evaluation harnesses, regression gates

Use this when: the team has strong foundational skills and established AI workflows.

### The Diagnostic Questions

Use these to determine the right ratio for a team:

1. **Can team members explain *why* the AI's output is correct or incorrect?** (If no → increase the 40%)
2. **Does the team write Context Packs before starting AI-directed work?** (If no → introduce the method at 40/60)
3. **Does the team document AI decisions in an Audit Log?** (If no → add the 10% reflection phase)
4. **Can team members catch AI errors before they reach production?** (If not consistently → increase the 40%)
5. **Is the team orchestrating multiple agents on complex projects?** (If yes → move toward 25/65/10)

---

## The Ratio Will Change

If AI capability continues accelerating, the floor of what AI handles autonomously will rise. The 40/60 split defined today may need to become 30/70 next year and 20/80 the year after.

This is expected and desirable. As AI becomes more capable, practitioners need less procedural practice and more evaluation/judgment practice.

**The principle is permanent: enough manual work to build informed judgment, then practice the real workflow.** The ratio is a dial, not a law.

### Review Cadence
- **Monthly:** Does the 40% content still represent the right foundational skills?
- **Quarterly:** Should the ratio shift based on team capability and AI tool evolution?
- **After each engagement:** What did the manual work teach that could not have been learned from the AI-directed work alone?

---

## Mapping to Studio Ordo Engagements

| Engagement | How the 40/60 Method Applies |
|------------|------------------------------|
| **Workshop (1-day)** | Session is structured as 40/60: morning is manual skill-building, afternoon is AI-directed practice with Context Packs |
| **Team Program (4-week)** | Week 1 heavy on manual foundations (50/50), Weeks 2–3 standard (40/60), Week 4 advanced (25/65/10) |
| **Advisory** | Consultant observes the team's current ratio, diagnoses gaps, and recommends a calibrated split with a transition plan |
| **Studio (Apprentice)** | Level 1: 50/50 — building foundations. Level 2: 40/60 — standard method. Level 3: 30/70 — increasing AI reliance. Level 4: 20/80 — near-professional workflow with deep evaluation skills |

---

## Connection to the Stripping Thesis

The 40/60 Method is the operational expression of the stripping thesis:

- The **40% manual work** represents the procedural skills that AI will eventually automate completely — but that practitioners must understand *now* to exercise judgment
- The **60% AI-directed work** represents the permanently human work: directing, evaluating, deciding, taking responsibility

As automation continues, the 40% shrinks and the 60% grows. The human work remains. That is the point.
