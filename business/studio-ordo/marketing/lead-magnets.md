# Lead Magnets — Specifications

## Overview

Three lead magnets, each designed to provide immediate value while introducing Studio Ordo's vocabulary, method, and frameworks. All are gated behind email capture. No fake scarcity. No countdown timers. The value is obvious; the exchange is fair.

**Funnel position:**
> Free content (EverydayAI, Ordo Brief) → Lead magnet download → Workshop booking or studio inquiry

---

## Lead Magnet 01 — The Spell Book: 40 Terms Every AI-Capable Engineer Knows

### Format
- PDF, 12–16 pages
- Clean typographic layout (Swiss grid, same brand as site)
- Print-friendly (engineers tape these to monitors)

### Content Structure

**Cover page:**
> The Spell Book
> 40 Terms Every AI-Capable Engineer Knows
> Studio Ordo

**Introduction (1 page):**
> Professional vocabulary creates precision. Precision creates communication. Communication creates teams that ship.
>
> This document contains 40 terms that define how AI-capable engineers think and work. They come from 23 years of teaching, 10,000 graduates, and the evidence-based frameworks behind Studio Ordo's training programs.
>
> Each term includes a working definition, why it matters, and an example of how it appears in practice. Start using them Monday morning.

**Terms organized in 5 categories (8 terms each):**

**Category 1 — Foundations**
1. Context Pack
2. AI Audit Log
3. Human Edge
4. 40/60 Ratio
5. Spell Book
6. Spec-Driven Development
7. Acceptance Criteria
8. Gate Project

**Category 2 — Inquiry & Judgment**
9. Disciplined Inquiry
10. Professional Judgment
11. Autodidactic Loop
12. Named Expert Critique
13. Problem Decomposition
14. Prompt Engineering
15. Evidence-Backed Claim
16. CLAIMS.md

**Category 3 — Resilience & Systems**
17. Failure Mode Analysis
18. Incident Drill
19. Defense-in-Depth
20. Error Budget
21. Orchestration
22. Command Pattern
23. Dependency Injection
24. STRIDE Model

**Category 4 — Data & Evaluation**
25. Epistemic Humility
26. Data Assumptions Document
27. Evaluation Criteria
28. Deployment Gap
29. Hybrid Data Stack
30. RAG (Retrieval-Augmented Generation)
31. Benchmark Saturation
32. Human-in-the-Loop

**Category 5 — Leadership & Translation**
33. Accountable Leadership
34. Translation Brief
35. Demo Day
36. Feynman Technique
37. Cognitive Load Theory
38. Field Report
39. Team Readiness Assessment
40. Maestro

**Per-term format:**

> **[Term]**
> *Definition:* [1-2 sentence working definition]
> *Why it matters:* [1 sentence connecting to professional practice]
> *In practice:* [1 sentence concrete example]

**Back page:**
> These 40 terms are a starting point. Studio Ordo's full professional vocabulary includes 60+ terms developed across workshops, team programs, and the apprenticeship studio.
>
> **View training tracks →** studioOrdo.com/services
> **Join the studio →** studioOrdo.com/studio

### Email Capture

**Landing page headline:**
> 40 terms. Zero jargon. Start using them Monday.

**Form:**
> - First name
> - Email
> - Role (dropdown: Engineer, Manager, VP/Director, Executive, Student, Other)

**Follow-up sequence:**
> - Day 0: Delivery email with PDF download link
> - Day 3: "3 terms to start with" email (Context Pack, AI Audit Log, 40/60 Ratio)
> - Day 7: Workshop invitation or Ordo Brief subscription prompt

---

## Lead Magnet 02 — The Context Pack Template Kit

### Format
- ZIP file containing 4 markdown templates + 1 example + 1 README
- Total: 6 files

### Contents

**README.md:**
> The Context Pack is your operating document for AI-assisted work. It tells both human collaborators and AI agents what they need to know about your project — scope, domain, constraints, evaluation criteria, and prior context.
>
> This kit includes templates for four levels of Context Pack complexity, plus a real example from a production project.

**Templates:**

1. **context-pack-v1-starter.md** — For individual projects and small features
   - Project brief (what, why, constraints)
   - Domain context (key terms, business rules)
   - Sections clearly labeled with [FILL IN] markers

2. **context-pack-v2-structured.md** — For team projects and features on existing systems
   - Everything in v1 +
   - Evaluation criteria (how do we know it's done?)
   - Agent instructions (what should AI tools know?)
   - Prior context (links to related Context Packs)

3. **context-pack-v3-complete.md** — For production systems and complex architectures
   - Everything in v2 +
   - Failure modes (what can go wrong?)
   - Orchestration spec (how do agents coordinate?)
   - Data assumptions (what are we assuming about our data?)

4. **context-pack-v4-production.md** — For client projects and organizational systems
   - Everything in v3 +
   - Prior context chain (full history of related work)
   - Stakeholder map (who needs to approve what?)
   - Incident response notes
   - Handoff documentation

5. **example-context-pack.md** — A completed v2 Context Pack for a real feature build
   - Filled in with realistic content
   - Annotations explaining each section's purpose

### Email Capture

**Landing page headline:**
> The document that makes AI agents useful. Four templates. One example. Free.

**Form:**
> - First name
> - Email
> - Team size (dropdown: Just me, 2-5, 6-20, 20+)

**Follow-up sequence:**
> - Day 0: Delivery email with ZIP download link
> - Day 3: "How to write your first Context Pack in 15 minutes" email
> - Day 7: Team Program overview or workshop invitation

---

## Lead Magnet 03 — The Human Edge Scorecard

### Format
- Interactive web page (self-assessment) + PDF results export
- 5 minutes to complete

### Content Structure

**Introduction:**
> Rate your current capability across the 8 Human Edge dimensions. This is a self-assessment — honest answers produce useful results.
>
> For each capability, rate yourself on a 4-point scale:
> - **0** — I'm not aware of this capability
> - **1** — I can describe it but haven't practiced it
> - **2** — I can demonstrate it with support
> - **3** — I can demonstrate it independently
> - **4** — I can teach it to others

**Assessment (8 questions):**

1. **Disciplined Inquiry** — Can you write a spec before writing code? Can you define what "done" looks like before you start?
2. **Professional Judgment** — When AI generates code, can you evaluate whether it's correct, appropriate, and production-ready? Do you have a process for accept/reject/modify decisions?
3. **Resilience Thinking** — Can you write a Failure Mode Analysis? Do you design for what breaks, not just for what works?
4. **Problem Finding** — Can you investigate an unfamiliar domain systematically? Can you distinguish the stated problem from the real problem?
5. **Epistemic Humility** — Do you document data assumptions? Can you articulate what your models and data cannot represent?
6. **Systems Thinking** — Can you design multi-component systems with clear interfaces? Do you understand how parts interact in ways that create emergent behavior?
7. **Accountable Leadership** — Can you present and defend technical decisions under questioning? Do you ship — not "almost done," but deployed and evaluated?
8. **Translation** — Can you explain complex technical concepts to non-technical audiences? Can you design learning experiences that manage cognitive load?

**Results page:**

> **Your Human Edge Score: [X] / 32**
>
> **Capability profile:** [Bar chart showing 8 scores]
>
> **Interpretation:**
> - 0–8: Foundation stage — start with Spell Book and Context Pack basics
> - 9–16: Developing — workshops would accelerate specific capabilities
> - 17–24: Proficient — team program or studio for advanced development
> - 25–32: Advanced — advisory or maestro-level engagement
>
> **Your strongest capability:** [Highest score]
> **Your development priority:** [Lowest score]
>
> **Recommended next step:** [Based on score range — maps to offer tier]

### Email Capture

**Gate: After completing assessment, before seeing results.**

**Prompt:**
> Enter your email to see your full Human Edge profile and get a personalized development recommendation.

**Form:**
> - First name
> - Email
> - Current role (dropdown: Junior Engineer, Mid-level Engineer, Senior Engineer, Tech Lead, Engineering Manager, VP/Director, Other)

**Follow-up sequence:**
> - Day 0: Results email with PDF export and development recommendation
> - Day 3: "Focus on your #1 development priority" email (tailored to lowest-scoring capability, links to relevant workshop)
> - Day 7: Ordo Brief subscription or workshop invitation

---

## Lead Magnet Distribution

| Lead Magnet | Primary Channel | Secondary Channel | Target Persona |
|------------|----------------|-------------------|---------------|
| Spell Book PDF | Homepage footer, services page | Ordo Brief, LinkedIn | Individual contributors |
| Context Pack Kit | Studio page, team program page | Workshop follow-up | Engineering managers |
| Human Edge Scorecard | About page, EverydayAI follow-up | Social media, newsletter | All personas |

## Voice Notes

- No fake urgency ("Download before it's gone!")
- No inflated value claims ("$500 value, yours free!")
- Value proposition is self-evident: professional tools that work
- Follow-up emails are useful content, not pitch sequences
- Unsubscribe is always one click, prominently placed
